---
import ActionsSection from "../../components/ActionsSection.astro";
import Goals from "../../components/Goals.astro";
import Summary from "../../components/Summary.astro";
import Prototype from "../../layouts/Prototype.astro";
---

<Prototype filename="vo-ios" title="Generate narration for any video" 
    subtitle="Take any video and generate a transcription and VoiceOver for it in any voice or style." 
    heroSrcArray={['/vo-ios/vo-ios-hero.png']}>

    <Summary 
    icons={["/ML.svg"]} 
    technologies={[
        "GPT-4 Turbo with Vision",
        "Whisper v3",
    ]} 
    tools={[
        "Xcode",
        "OpenAI API",
    ]} 
    features={[
        "Generate VoiceOver for any video",
        "Choose from a variety of voices and styles",
        "Transcribe videos",
    ]} 
    developers={[
        { name: "Tom Holmes", website: "https://github.com/tommy-holmes" },
    ]}
    />

    <Goals title="The why" goals={[
        "See if we could make video content more accessible to those with visual impairments",
        "Allow creators to generate VoiceOver for their videos in any voice or style",
        "Push GPT-4 Turbo with Vision to its limits by combining it with Whisper and see what use cases we could come up with",
    ]} />

<ActionsSection actions={[
    {
        title: "Transcribing a video with GPT-4's Vision capabilities",
        paragraphs: [
            "The first step is to upload a video to the app. The video is then transformed into a series of static images and sent to GPT-4 Turbo with Vision for transcription.",
            "This is because, currently, GPT-4 Turbo cannot transcribe videos directly, so we have to break it down into a series of images first, like a slideshow verison of the video.",
            "The idea is to give GPT-4 enough context, along with any user settings like style, to understand the video and generate detailed a transcription.",
        ],
        imgSrc: '/vo-ios/empty-state.png'
    },
]} />

</Prototype>
 